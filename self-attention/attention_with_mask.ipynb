{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, qkv_bias = False):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert self.embedding_dim % self.num_heads == 0, \"embedding_dim should be divisible by num of heads\"\n",
    "\n",
    "        self.wq = nn.Linear(self.embedding_dim, self.embedding_dim, bias = qkv_bias)\n",
    "        self.wk = nn.Linear(self.embedding_dim, self.embedding_dim, bias = qkv_bias)\n",
    "        self.wv = nn.Linear(self.embedding_dim, self.embedding_dim, bias = qkv_bias)\n",
    "\n",
    "        # self.softmax = F.softmax(dim = -1)\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        batch_size, sequence_length, channel = query.shape\n",
    "\n",
    "        query = self.wq(query)  # [b, L, c]\n",
    "        key = self.wk(key)\n",
    "        value = self.wv(value)\n",
    "\n",
    "        query = query.view([batch_size, sequence_length, self.num_heads, -1]).transpose(1,2)  # [b, L, c] -> [b, num_heads, L, c//num_heads]\n",
    "        key = key.view([batch_size, sequence_length, self.num_heads, -1]).transpose(1,2)\n",
    "        value = value.view([batch_size, sequence_length, self.num_heads, -1]).transpose(1,2)\n",
    "\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1,-2)) * channel ** -0.5\n",
    "\n",
    "        if mask is not None:\n",
    "            if mask.dim() == 3:\n",
    "                mask = mask.unsqueeze(1)  # Shape: [batch_size, 1, seq_length, seq_length]\n",
    "            mask = mask.to(torch.bool)\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "            \n",
    "        attention_scores = F.softmax(attention_scores, dim = -1)\n",
    "        attention_output = torch.matmul(attention_scores, value)\n",
    "\n",
    "        attention_output = attention_output.view([batch_size, sequence_length, -1])\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 77, 768])\n"
     ]
    }
   ],
   "source": [
    "batch_size, sequence_length, embedding_dim, num_heads = 2, 77, 768, 8\n",
    "\n",
    "query = torch.randn([batch_size, sequence_length, embedding_dim])\n",
    "key = torch.randn([batch_size, sequence_length, embedding_dim])\n",
    "value = torch.randn([batch_size, sequence_length, embedding_dim])\n",
    "\n",
    "mask = torch.ones([batch_size, sequence_length, sequence_length])\n",
    "mask[:, :, sequence_length // 2 :] = 0\n",
    "\n",
    "model = SelfAttention(embedding_dim = embedding_dim, num_heads = num_heads)\n",
    "output = model(query, key, value, mask)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attention-mask的作用是什么？\n",
    "\n",
    "1. 首先在自注意力机制中, attention-mask的作用主要是为了控制模型在计算自注意力时, 对某些位置的特征进行关注和忽略。\n",
    "2. attention-mask是一个矩阵, 用于在注意力计算中掩盖掉某些位置。通过, mask中的掩码位置值为一个非常大的负数, 以确保这些位置在softmax后接近0。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
