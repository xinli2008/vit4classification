{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_head):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_head = num_head\n",
    "        self.head_dim = self.embedding_dim // self.num_head\n",
    "\n",
    "        assert self.embedding_dim == self.num_head * self.head_dim, \"embedding dimension should be divisible by num of heads\"\n",
    "        self.wq = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
    "        self.wk = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
    "        self.wv = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
    "\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        batch_size, sequence_length, embedding_dim = query.shape\n",
    "        \n",
    "        # [b, num_head, seq_len, head_dim]\n",
    "        query = query.view(batch_size, sequence_length, self.head_dim, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        key = key.view(batch_size, sequence_length, self.head_dim, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        value = value.view(batch_size, sequence_length, self.head_dim, -1).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        query = self.wq(query)\n",
    "        key = self.wk(key)\n",
    "        value = self.wv(value)\n",
    "\n",
    "        attention_score = torch.matmul(query, key.transpose(2,3)) * (self.head_dim ** -0.5)\n",
    "        attention_score = F.softmax(attention_score, dim = -1)\n",
    "        attention_out = torch.matmul(attention_score, value)\n",
    "        attention_out = attention_out.transpose(1,2).flatten(2)\n",
    "        \n",
    "        return attention_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = torch.randn([1, 3, 80])\n",
    "key = torch.randn([1, 3, 80])\n",
    "value = torch.randn([1, 3, 80])\n",
    "num_heads = 8\n",
    "embedding_dim = 80\n",
    "multi_head_self_attention = MultiHeadSelfAttention(embedding_dim = 80, num_head = 10)\n",
    "out = multi_head_self_attention(query, key, value)\n",
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
